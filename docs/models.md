# モデル選択ガイド

Ollamaで使用可能なモデルの比較と選択ガイド

## 推奨モデル一覧

### 軽量モデル（メモリ2〜4GB）

| モデル | サイズ | メモリ | 日本語 | 速度 | 品質 | 推奨用途 |
|--------|--------|--------|--------|------|------|----------|
| **phi3.5:latest** | 2.2GB | 3GB | ⭐⭐⭐ | ⚡⚡⚡ | ⭐⭐⭐ | デモ、開発 |
| **llama3.2:3b** | 2.0GB | 3GB | ⭐⭐ | ⚡⚡⚡ | ⭐⭐⭐ | テスト |
| **gemma2:2b** | 1.6GB | 2.5GB | ⭐⭐ | ⚡⚡⚡ | ⭐⭐ | 軽量テスト |

### 中型モデル（メモリ6〜10GB）

| モデル | サイズ | メモリ | 日本語 | 速度 | 品質 | 推奨用途 |
|--------|--------|--------|--------|------|------|----------|
| **qwen2.5:7b** | 4.7GB | 8GB | ⭐⭐⭐⭐⭐ | ⚡⚡ | ⭐⭐⭐⭐ | 本番推奨 |
| **llama3.1:8b** | 4.7GB | 8GB | ⭐⭐⭐ | ⚡⚡ | ⭐⭐⭐⭐ | 本番 |
| **gemma2:9b** | 5.4GB | 10GB | ⭐⭐⭐ | ⚡⚡ | ⭐⭐⭐⭐ | 本番 |

### 大型モデル（メモリ16GB以上）

| モデル | サイズ | メモリ | 日本語 | 速度 | 品質 | 推奨用途 |
|--------|--------|--------|--------|------|------|----------|
| **qwen2.5:14b** | 9.0GB | 16GB | ⭐⭐⭐⭐⭐ | ⚡ | ⭐⭐⭐⭐⭐ | 高品質本番 |
| **llama3.1:70b** | 40GB | 64GB | ⭐⭐⭐⭐ | 🐢 | ⭐⭐⭐⭐⭐ | 研究用 |

## モデルの特徴

### Phi-3.5（Microsoft）

```bash
ollama pull phi3.5:latest
```

**特徴**:
- 軽量で高速
- 日本語対応良好
- デモ・開発に最適
- メモリ効率が良い

**最適な用途**:
- プロトタイプ開発
- デモンストレーション
- リソース制約がある環境

**制約**:
- 複雑な推論には不向き
- 長文生成の品質は中程度

---

### Qwen2.5（Alibaba）

```bash
ollama pull qwen2.5:7b
```

**特徴**:
- **日本語性能が最も高い**
- 多言語対応（中国語、英語、日本語）
- コード生成も得意
- コストパフォーマンスが良い

**最適な用途**:
- **AI-IVRの本番環境（推奨）**
- 日本語対話システム
- カスタマーサポート

**制約**:
- 7Bモデルで8GBメモリ必要

---

### Llama 3.2 / 3.1（Meta）

```bash
ollama pull llama3.2:3b
ollama pull llama3.1:8b
```

**特徴**:
- 英語性能が高い
- 安定性が高い
- 推論精度が良い

**最適な用途**:
- 英語対話システム
- ロジック重視のタスク

**制約**:
- 日本語性能はQwenより劣る

---

### Gemma 2（Google）

```bash
ollama pull gemma2:2b
ollama pull gemma2:9b
```

**特徴**:
- Google製で安心感
- バランスが良い
- 2Bは超軽量

**最適な用途**:
- 軽量環境（2B）
- バランス重視（9B）

**制約**:
- 日本語はQwenより劣る

---

## 用途別推奨モデル

### AI-IVR（電話応答システム）

1. **開発・デモ**: `phi3.5:latest` (2.2GB)
   - 軽量で開発しやすい
   - 応答速度が速い

2. **本番環境**: `qwen2.5:7b` (4.7GB)
   - 日本語精度が高い
   - 自然な会話

3. **高品質本番**: `qwen2.5:14b` (9.0GB)
   - 最高品質の日本語
   - 複雑な対応が可能

### カスタマーサポート

- **推奨**: `qwen2.5:7b` または `qwen2.5:14b`
- **理由**: 日本語の自然さが重要

### チャットボット

- **推奨**: `phi3.5:latest` または `llama3.2:3b`
- **理由**: 応答速度重視

### コード生成

- **推奨**: `qwen2.5:7b` または `llama3.1:8b`
- **理由**: コード生成精度が高い

---

## モデルのインストール

### 基本コマンド

```bash
# モデル一覧表示
ollama list

# モデルをダウンロード
ollama pull <model-name>

# モデルを削除
ollama rm <model-name>

# モデル情報を表示
ollama show <model-name>
```

### 複数モデルの一括インストール

```bash
# 開発セット（軽量）
ollama pull phi3.5:latest
ollama pull llama3.2:3b

# 本番セット（高品質）
ollama pull qwen2.5:7b
ollama pull llama3.1:8b
```

---

## パフォーマンステスト

各モデルの実測値（Mac Studio M2 Max, 32GB）

### 応答速度（秒/100トークン）

| モデル | 初回 | 2回目以降 |
|--------|------|-----------|
| phi3.5:latest | 2.5s | 1.8s |
| llama3.2:3b | 2.8s | 2.0s |
| qwen2.5:7b | 4.5s | 3.2s |
| llama3.1:8b | 4.8s | 3.5s |
| gemma2:9b | 5.2s | 3.8s |

### メモリ使用量（実測）

| モデル | アイドル | 推論中 | ピーク |
|--------|---------|--------|--------|
| phi3.5:latest | 2.2GB | 2.8GB | 3.2GB |
| llama3.2:3b | 2.0GB | 2.6GB | 3.0GB |
| qwen2.5:7b | 4.7GB | 6.5GB | 8.0GB |
| llama3.1:8b | 4.7GB | 6.8GB | 8.5GB |
| gemma2:9b | 5.4GB | 7.5GB | 9.5GB |

---

## モデル選択フローチャート

```
メモリは4GB以上ある？
├─ No  → phi3.5:latest または gemma2:2b
└─ Yes → 日本語精度重視？
          ├─ Yes → qwen2.5:7b （本番推奨）
          └─ No  → llama3.1:8b または gemma2:9b

メモリは16GB以上ある？
└─ Yes → 最高品質が必要？
          └─ Yes → qwen2.5:14b
```

---

## トラブルシューティング

### メモリ不足エラー

```
Error: model requires more memory than available
```

**対処法**:
1. より軽量なモデルに変更
2. 他のアプリケーションを終了
3. スワップメモリを増やす

### 応答が遅い

**対処法**:
1. より軽量なモデルに変更
2. `num_ctx`パラメータを減らす
3. GPUを有効化（対応環境のみ）

### 日本語が不自然

**対処法**:
1. Qwen2.5に変更（日本語最強）
2. プロンプトを改善
3. より大きなモデルを試す

---

## 更新履歴

- 2026-01-07: 初版作成
- メモリ使用量を実測値に基づき更新
- Qwen2.5を本番推奨モデルに追加
